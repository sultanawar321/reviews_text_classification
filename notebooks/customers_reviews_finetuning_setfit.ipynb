{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQf1baks4GcZ"
      },
      "source": [
        "- This notebook fine-tunes the SetFit (Sentence Transfromers) Hugging face model on a small labelled dataset of customers reviews to solve  a multiclassifcation problem and generate topics/themes for unlabelled data.\n",
        "\n",
        "- It also includes a quick fine-tuning demo on sample data from Datasets library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXsGvFf65fPB",
        "outputId": "ae9948ee-4b02-4690-ac2d-936b7bcd4e4c"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install setfit==1.0.3\n",
        "!pip install loguru==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8AVKinv4Gcb"
      },
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import os\n",
        "from datetime import date\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from loguru import logger\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "from setfit import SetFitModel, Trainer, TrainingArguments\n",
        "from datasets import Dataset,load_dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4jvsWMa4Gcd"
      },
      "outputs": [],
      "source": [
        "# Set Main Variables\n",
        "SETFIT_TRAINING_MODEL = 'sentence-transformers/paraphrase-mpnet-base-v2'\n",
        "TEST_SIZE = 0.15\n",
        "Training_Arguments = TrainingArguments(\n",
        "    batch_size=16, # num samples are passed through the model at once before updating the model's parameters\n",
        "    num_epochs=1, # num of complete passes through training data\n",
        "    seed = 42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IrWAugu4Gcd"
      },
      "source": [
        "### 1. Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nSUJJT-xK0b"
      },
      "outputs": [],
      "source": [
        "def data_prep():\n",
        "    \"\"\"\n",
        "    Reads the raw data from a CSV file, preprocesses it,\n",
        "    and returns aa a DataFrame.\n",
        "    \"\"\"\n",
        "    # Prompt user to upload the reviews csv file\n",
        "    logger.info('Uploading the labeled reviews dataset from local machine')\n",
        "    uploaded = files.upload()\n",
        "    # Read the csv file as pandas df\n",
        "    df_raw = pd.read_csv('labelled_reviews.csv')\n",
        "    df_raw = df_raw[['text', 'label']]\n",
        "    # Drop null values if any\n",
        "    df_raw = df_raw.dropna(subset=['text', 'label'])\n",
        "    df_raw['label'] = df_raw['label'].str.strip()\n",
        "    # Apply a numeric mapping for the categories\n",
        "    category_mapping = {\n",
        "        'Flights and Departures': 1,\n",
        "        'Entertainment and Food': 2,\n",
        "        'Cabin Comfort and Baggage': 3,\n",
        "        'Lounge Experience': 4,\n",
        "        'Boarding and Crew Experience': 5,\n",
        "        'Bookings and Refunds': 6}\n",
        "    df_raw['label'] = df_raw['label'].map(category_mapping)\n",
        "    # Re sample the data\n",
        "    df_raw = df_raw.sample(frac=1)\n",
        "    return df_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload, load, and prepare the reviews dataset\n",
        "df_training = data_prep()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ix1R2Jx94Gce"
      },
      "source": [
        "### 2. Split Train/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGbREjJYzGpN"
      },
      "outputs": [],
      "source": [
        "def data_split_train_test(responses, test_size):\n",
        "    \"\"\"\n",
        "    This function splits the dataset into train and validation based on test size\n",
        "    \"\"\"\n",
        "    responses.dropna(subset=['label'], inplace=True)\n",
        "    train, val = train_test_split(responses, test_size=test_size,\n",
        "                                  random_state=42, shuffle=True, stratify=responses['label'])\n",
        "    train_pd, val_pd = map(lambda x: x.reset_index(drop=True), [train, val,])\n",
        "    return train_pd, val_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply train_test split\n",
        "train_pd, val_pd = data_split_train_test(df_training, TEST_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AykJLGoi4Gce"
      },
      "source": [
        "### 3. Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cN4jCSX0xor"
      },
      "outputs": [],
      "source": [
        "def data_transformation(train_pd, val_pd):\n",
        "    \"\"\"\n",
        "    This function transforms the pandas to dataset, a format accepted by the transformers\n",
        "    \"\"\"\n",
        "    # transform from pandas into datasets format (accpeted by the transfomers)\n",
        "    train = Dataset.from_pandas(train_pd)\n",
        "    val = Dataset.from_pandas(val_pd)\n",
        "    return train, val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform data to datasets format\n",
        "train_data, val_data = data_transformation(train_pd, val_pd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voimf9Kg4Gcf"
      },
      "source": [
        "### 4. Apply fine-tuning on labeled customers reviews dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiHetSaK4Gcf"
      },
      "outputs": [],
      "source": [
        "def model_finetuning(MODEL, TrainingArguments, Train_Data, Val_Data):\n",
        "    \"\"\"\n",
        "    Fine-tunes the specified model on the provided training data and evaluates it on the validation data.\n",
        "    \"\"\"\n",
        "    # Initiate the model\n",
        "    model = SetFitModel.from_pretrained(MODEL)\n",
        "    trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments,\n",
        "    train_dataset=Train_Data,\n",
        "    eval_dataset= Val_Data\n",
        "    )\n",
        "    logger.info('fine-tuning the Setfit model on dataset')\n",
        "    # Start the finetuning job\n",
        "    trainer.train()\n",
        "    logger.info('saving the fine-tuned model')\n",
        "    model_directory_timestamp = f'{date.today().strftime(\"%Y%m%d\")}-reviews-text-classification'\n",
        "    # Save the model locally\n",
        "    trainer.model.save_pretrained(model_directory_timestamp)\n",
        "    # Evaluate the model\n",
        "    metrics = trainer.evaluate()\n",
        "    logger.info(f\"Performance of the fine-tuned model: , {metrics}\")\n",
        "    return trainer, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply the model_fine_tuning function on the customers reviews train and val data\n",
        "# This needs some GPU power to get completed due to data size\n",
        "trainer, metrics = model_finetuning(\n",
        "        SETFIT_TRAINING_MODEL, Training_Arguments, train_data, val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVLUqPLaafJ7"
      },
      "source": [
        "### 5. Run a Fine-tuning demo on sample dataset from Datasets Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CGBr6WMYaK8"
      },
      "outputs": [],
      "source": [
        "def data_load():\n",
        "    \"\"\"\n",
        "    Load a sample dataset from Datasets Library\n",
        "    \"\"\"\n",
        "    # Load the dataset from datasets library\n",
        "    dataset = load_dataset(\"SetFit/SentEval-CR\")\n",
        "    # Select N examples per class (8 in this case) for the train dataset\n",
        "    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(8 * 2))\n",
        "    # Get the test dataset\n",
        "    test_ds = dataset[\"test\"]\n",
        "    return train_ds, test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the train and val datasets for a quick demo\n",
        "train_ds, val_ds = data_load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dq9eJRyYuoh"
      },
      "outputs": [],
      "source": [
        "# Apply the model fine-tuning function on the demo train and val datasets\n",
        "# This runs perfectly with CPU\n",
        "trainer_demo, metrics_demo = model_finetuning(\n",
        "        SETFIT_TRAINING_MODEL, Training_Arguments, train_ds, val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Push the fine-tuned model to hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# excute this cell to paste the API access token generated from HuggingFace account and push the fine-tuned model to hub\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add the repo name followed by the model name\n",
        "trainer_demo.push_to_hub(\"sultanaw/customer_reviews_setfit\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
