{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":923,"status":"ok","timestamp":1714378765620,"user":{"displayName":"Sultan Al Awar","userId":"09630990462177093846"},"user_tz":-60},"id":"2njqrYkDkE-x","outputId":"b5b89f73-e0b4-4c8b-8452-c2e1ce4ba63d"},"outputs":[],"source":["# Standard Libraries\n","import pandas as pd\n","import re\n","import string\n","import os\n","\n","# Natural Language Processing Libraries\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","# Machine Learning Libraries\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Topic Modeling Libraries\n","import gensim\n","import gensim.corpora as corpora\n","from gensim.corpora import Dictionary\n","from gensim.models import LdaModel\n","\n","# Visualisation libraries\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","# Downloading NLTK Packages\n","#nltk.download(['stopwords', 'wordnet', 'punkt'])\n","\n","# Define Stop Words\n","stop_words = list(stopwords.words('english'))\n","\n","# Add Custom Stop Words\n","new_words = ['british', 'airway', 'company', 'airline', 'flight', 'heathrow', 'service', 'london',\n","             'business', 'economy', 'customer', 'passenger', 'hour', 'minute']\n","stop_words.extend(new_words)\n"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":238,"status":"ok","timestamp":1714378766840,"user":{"displayName":"Sultan Al Awar","userId":"09630990462177093846"},"user_tz":-60},"id":"Vu7yh7ffkFcf"},"outputs":[],"source":["def load_data() -> pd.DataFrame:\n","  \"\"\"\n","  this fucnction loads the data from csv file and samples 20% of the data\n","  \"\"\"\n","  data = pd.read_csv('reviews.csv')\n","  data = data[['ReviewBody']]\n","  data = data.sample(frac=0.20, random_state=42).reset_index(drop=True)  \n","  return data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":195,"status":"ok","timestamp":1714378772423,"user":{"displayName":"Sultan Al Awar","userId":"09630990462177093846"},"user_tz":-60},"id":"7B-uvR3jjPyI"},"outputs":[],"source":["df_reviews = load_data()"]},{"cell_type":"markdown","metadata":{},"source":["### 2. Data Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1714378773923,"user":{"displayName":"Sultan Al Awar","userId":"09630990462177093846"},"user_tz":-60},"id":"oosPfw9Wj6ii"},"outputs":[],"source":["def preprocess_text(text : str) -> list:\n","    \"\"\"\n","    This function prepares the text data, conducting the following steps:\n","    1) Removal of text in sqaure brackets\n","    2) Removal of words containing numbers\n","    3) Removal of emojis\n","    4) Removal of extra spaces and newline characters\n","    5) Tokenization\n","    6) Lemmatization\n","    7) Removal of stopwords\n","    8) Removal of punctuation\n","    9) Removal of names\n","    \"\"\"\n","    # remove text in square brackets\n","    text = re.sub(r'\\[.*?\\]', '', text)\n","    # remove special characters\n","    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n","    # Remove words containing numbers\n","    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n","    # Remove emojis\n","    text = text.encode('ascii', 'ignore').decode('ascii')\n","    # Remove extra spaces and newline characters\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    # Tokenize the text\n","    tokens = word_tokenize(text.lower())\n","    # Initialize WordNet lemmatizer\n","    lemmatizer = WordNetLemmatizer()\n","    # Lemmatize tokens\n","    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","    # Remove stop words and punctuation\n","    processed_tokens = [token for token in lemmatized_tokens if token.lower() not in stop_words and token not in string.punctuation and len(token)>5]\n","    return processed_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3222,"status":"ok","timestamp":1714378782344,"user":{"displayName":"Sultan Al Awar","userId":"09630990462177093846"},"user_tz":-60},"id":"QJFW5nXrk9UA"},"outputs":[],"source":["df_reviews['ReviewBody'] = df_reviews.ReviewBody.apply(\n","    lambda x: preprocess_text(x))  # apply the function"]},{"cell_type":"markdown","metadata":{},"source":["### 3. Bag of words and LDA (Gensim)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def apply_lda_with_bag_of_words(df_reviews:pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"\n","    This function applies LDA from gensim with bag of words approach. \n","    \"\"\"\n","    # Map IDs to words to be used as an input for the LDA model using the universal corpous ids\n","    # this approach uses word counts and word positions\n","    words = corpora.Dictionary(df_reviews['ReviewBody'])\n","    \n","    # Turn each review into a bag of words.\n","    corpus = [words.doc2bow(doc) for doc in df_reviews['ReviewBody']]\n","    \n","    # Apply the LDA model from gensim to establish topics\n","    lda_bag_of_words_model = gensim.models.ldamodel.LdaModel(\n","        corpus=corpus,\n","        id2word=words,\n","        num_topics=6,\n","        random_state=42,\n","        update_every=1,\n","        passes=12,\n","        alpha='auto'\n","    )\n","    \n","    feature_names_bag_of_words = list(words.values())\n","    n_top_words = 10\n","    \n","    # Initialize a dictionary to store topic words\n","    topics_dict = {}\n","    \n","    for topic_idx, topic in enumerate(lda_bag_of_words_model.get_topics()):\n","        topic_words = [feature_names_bag_of_words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n","        topics_dict[f'Topic {(topic_idx + 1)}'] = topic_words\n","    \n","    # Turn the dict into dataframe\n","    lda_bag_of_words_model_topics_df = pd.DataFrame(topics_dict)\n","    \n","    return feature_names_bag_of_words, lda_bag_of_words_model, lda_bag_of_words_model_topics_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature_names_bag_of_words,  lda_bag_of_words_model, lda_with_bag_of_words_topics_df = apply_lda_with_bag_of_words(df_reviews)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"elapsed":5179,"status":"ok","timestamp":1714379818444,"user":{"displayName":"Sultan Al Awar","userId":"09630990462177093846"},"user_tz":-60},"id":"Cd_rJod29AXF","outputId":"0a5e4be8-63e6-4dbf-9cdb-f6c21f5e9ad1"},"outputs":[],"source":["def visualize_topics(model_method: str, feature_names:list, n_top_words:int) -> None:\n","    \"\"\"\n","    Visualizes topics generated by a topic modeling method using word clouds.\n","    \"\"\"\n","    # Create subplots for each topic\n","    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(20, 8), sharex=True, sharey=True)\n","    axes = axes.flatten()\n","\n","    for topic_idx, topic in enumerate(model_method):\n","        # Generate word cloud for each topic\n","        wordcloud = WordCloud(background_color=\"white\", colormap=\"viridis\").generate(' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n","        ax = axes[topic_idx]\n","        ax.imshow(wordcloud, interpolation='bilinear')\n","        ax.set_title(f'Topic {topic_idx + 1}', fontsize=16)\n","        ax.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualize_topics(lda_bag_of_words_model.get_topics(), feature_names_bag_of_words, n_top_words=10)"]},{"cell_type":"markdown","metadata":{"id":"DdJQP631h8Fc"},"source":["## Topics from LDA and Bag of Words\n","1) Food and Baverages\n","2) Cabin and Luggage\n","3) Entertaiment\n","4) Departures and Flights\n","5) Booking and Lounge experience\n","6) Lounge and Boarding"]},{"cell_type":"markdown","metadata":{},"source":["### 4. TFIDF and LDA (scikit-learn)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def apply_lda_with_tfidf(df_reviews):\n","    \"\"\"\n","    This function applies LDA from scikit-learn with tfidf approach. \n","    \"\"\"\n","    # Initialize TFIDF\n","    tfidf = TfidfVectorizer(max_df=.8, min_df=20, max_features=10000)\n","    # Convert the feedback_answer_tokenized column to a list of strings\n","    df_reviews['ReviewBody_tokenized'] = df_reviews['ReviewBody'].apply(\n","    lambda x: ' '.join(x))\n","    # Fit the TF-IDF vectorizer to the data\n","    X = tfidf.fit_transform(df_reviews['ReviewBody_tokenized'])\n","    # Run LDA\n","    lda_model_with_tfidf = LatentDirichletAllocation(\n","    n_components=6, random_state=123, learning_method='batch')\n","    lda_model_with_tfidf.fit_transform(X)\n","    \n","    n_top_words = 10\n","    feature_names_tfidf = tfidf.get_feature_names_out()\n","    \n","    # Initialize a dictionary to store topic words\n","    topics_dict = {}\n","    \n","    for topic_idx, topic in enumerate(lda_model_with_tfidf.components_):\n","        topic_words = [feature_names_tfidf[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n","        topics_dict[f'Topic {(topic_idx + 1)}'] = topic_words\n","    \n","    # Turn the dict into dataframe\n","    lda_tfidf_topics_df = pd.DataFrame(topics_dict)\n","    return feature_names_tfidf, lda_model_with_tfidf, lda_tfidf_topics_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature_names_tfidf, lda_model_with_tfidf, lda_tfidf_topics_df = apply_lda_with_tfidf(df_reviews)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["visualize_topics(lda_model_with_tfidf.components_, feature_names_tfidf, n_top_words=10)"]},{"cell_type":"markdown","metadata":{"id":"piIQOrNWmRA6"},"source":["## Topics from LDA and TFIDF\n","1) Lounge Experience\n","2) Bookings \n","3) Departures, Ticketing and Cancellations\n","4) Luggage Handling and Delays\n","5) Cabin and Crew Experience\n","6) Baggage and Boarding"]},{"cell_type":"markdown","metadata":{"id":"7UqJlq_BtCMn"},"source":["## **Final list of Topics:**\n","\n","1) Boarding and Crew Experience\n","2) Entertainment and Food\n","3) Cabin Comfort and Baggage\n","4) Lounge Experience\n","5) Bookings and Refunds\n","6) Flights and Cancellations"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMZX1kBTehMUbHLPRc7AZbw","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
