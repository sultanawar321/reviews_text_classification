{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2njqrYkDkE-x",
        "outputId": "3c64ca2a-fd63-4e39-f515-fac2f0c363f0"
      },
      "outputs": [],
      "source": [
        "# Standard Libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Natural Language Processing Libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# SKlearn Libraries\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Gensim Libraries\n",
        "import gensim\n",
        "import gensim.corpora as corpora\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "\n",
        "# Visualisation libraries\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Downloading NLTK Packages\n",
        "nltk.download(['stopwords', 'wordnet', 'punkt'])\n",
        "\n",
        "# Define Stop Words\n",
        "stop_words = list(stopwords.words('english'))\n",
        "\n",
        "# Add Custom Stop Words\n",
        "new_words = ['british', 'airway', 'company', 'airline', 'flight', 'heathrow', 'service', 'london',\n",
        "             'business', 'economy', 'customer', 'passenger', 'hour', 'minute']\n",
        "stop_words.extend(new_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfkJjjoXNGDf"
      },
      "source": [
        "### 1. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu7yh7ffkFcf"
      },
      "outputs": [],
      "source": [
        "def load_data() -> pd.DataFrame:\n",
        "  \"\"\"\n",
        "  The function:\n",
        "  - uploads the data from local machine\n",
        "  - read the data from csv file to df\n",
        "  - samples 20% of the data\n",
        "  \"\"\"\n",
        "  # Prompt user to upload a the reviews csv file\n",
        "  uploaded = files.upload()\n",
        "  # Read the data from csv\n",
        "  data = pd.read_csv('reviews.csv')\n",
        "  data = data[['ReviewBody']]\n",
        "  # Sample 20% of the data\n",
        "  data = data.sample(frac=0.20, random_state=42).reset_index(drop=True)\n",
        "  return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "7B-uvR3jjPyI",
        "outputId": "436ac7fb-a0f0-4c4d-be26-6a56c88d9080"
      },
      "outputs": [],
      "source": [
        "df_reviews = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBPQic7RNGDi"
      },
      "source": [
        "### 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oosPfw9Wj6ii"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text : str) -> list:\n",
        "    \"\"\"\n",
        "    This function prepares the text data, conducting the following steps:\n",
        "    1) Removal of text in sqaure brackets\n",
        "    2) Removal of words containing numbers\n",
        "    3) Removal of emojis\n",
        "    4) Removal of extra spaces and newline characters\n",
        "    5) Tokenization\n",
        "    6) Lemmatization\n",
        "    7) Removal of stopwords\n",
        "    8) Removal of punctuation\n",
        "    9) Removal of names\n",
        "    \"\"\"\n",
        "    # Remove text in square brackets\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    # Remove special characters\n",
        "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
        "    # Remove words containing numbers\n",
        "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
        "    # Remove emojis\n",
        "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
        "    # Remove extra spaces and newline characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Initialize WordNet lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    # Lemmatize tokens\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    # Remove stop words and punctuation\n",
        "    processed_tokens = [token for token in lemmatized_tokens if token.lower() not in stop_words and token not in string.punctuation and len(token)>5]\n",
        "    return processed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJFW5nXrk9UA"
      },
      "outputs": [],
      "source": [
        "df_reviews['ReviewBody'] = df_reviews.ReviewBody.apply(\n",
        "    lambda x: preprocess_text(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcRZ8fpbNGDj"
      },
      "source": [
        "### 3. Bag of words and LDA (Gensim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKsYNQZ_NGDk"
      },
      "outputs": [],
      "source": [
        "def apply_lda_with_bag_of_words(df_reviews:pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    This function applies LDA from gensim with bag of words approach.\n",
        "    \"\"\"\n",
        "    # Map IDs to words to be used as an input for the LDA model using the universal corpous ids\n",
        "    words = corpora.Dictionary(df_reviews['ReviewBody'])\n",
        "\n",
        "    # Turn each review into a bag of words.\n",
        "    corpus = [words.doc2bow(doc) for doc in df_reviews['ReviewBody']]\n",
        "\n",
        "    # Apply the LDA model from gensim to establish topics\n",
        "    lda_bag_of_words_model = gensim.models.ldamodel.LdaModel(\n",
        "        corpus=corpus, #Â text\n",
        "        id2word=words, # representations\n",
        "        num_topics=6, # define number of topics\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    feature_names_bag_of_words = list(words.values()) # get the features names from Bag of Words\n",
        "    n_top_words = 10\n",
        "\n",
        "    # Initialize a dictionary to store dominant words per LDA topic group\n",
        "    topics_dict = {}\n",
        "\n",
        "    for topic_idx, topic in enumerate(lda_bag_of_words_model.get_topics()):\n",
        "        topic_words = [feature_names_bag_of_words[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
        "        topics_dict[f'Topic {(topic_idx + 1)}'] = topic_words\n",
        "\n",
        "    # Turn the dict of topics/words into df\n",
        "    lda_bag_of_words_model_topics_df = pd.DataFrame(topics_dict)\n",
        "\n",
        "    return feature_names_bag_of_words, lda_bag_of_words_model, lda_bag_of_words_model_topics_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYhdSUd2NGDk",
        "outputId": "d4a4f977-d91d-4ea8-dcfd-8e6b8fee229e"
      },
      "outputs": [],
      "source": [
        "feature_names_bag_of_words,  lda_bag_of_words_model, lda_with_bag_of_words_topics_df = apply_lda_with_bag_of_words(df_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd_rJod29AXF"
      },
      "outputs": [],
      "source": [
        "def visualize_topics(model_method: str, feature_names:list, n_top_words:int) -> None:\n",
        "    \"\"\"\n",
        "    Visualizes topics generated by a topic modeling method using word clouds.\n",
        "    \"\"\"\n",
        "    # Create subplots for each topic\n",
        "    fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 8), sharex=True, sharey=True)\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for topic_idx, topic in enumerate(model_method):\n",
        "        # Generate word cloud for each topic\n",
        "        wordcloud = WordCloud(background_color=\"white\", colormap=\"viridis\").generate(' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
        "        ax = axes[topic_idx]\n",
        "        ax.imshow(wordcloud, interpolation='bilinear')\n",
        "        ax.set_title(f'Topic {topic_idx + 1}', fontsize=16)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "lKLXpxuzNGDl",
        "outputId": "28a054bc-2f01-443e-88b7-a668ecc3bb2b"
      },
      "outputs": [],
      "source": [
        "visualize_topics(lda_bag_of_words_model.get_topics(), feature_names_bag_of_words, n_top_words=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdJQP631h8Fc"
      },
      "source": [
        "## Topics from LDA (Gensim) and Bag of Words Method\n",
        "1) Food and Baverages\n",
        "\n",
        "2) Cabin and Luggage\n",
        "\n",
        "3) Entertaiment\n",
        "\n",
        "4) Departures and Flights\n",
        "\n",
        "5) Booking\n",
        "\n",
        "6) Lounge and Boarding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HTlkERdNGDm"
      },
      "source": [
        "### 4. TFIDF and LDA (scikit-learn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW4hpB1RNGDm"
      },
      "outputs": [],
      "source": [
        "def apply_lda_with_tfidf(df_reviews):\n",
        "    \"\"\"\n",
        "    This function applies LDA from scikit-learn with tfidf approach.\n",
        "    \"\"\"\n",
        "    # Initialize TFIDF\n",
        "    tfidf = TfidfVectorizer(max_df=.8, min_df=20, max_features=10000)\n",
        "    # Convert the text column to a list of strings\n",
        "    df_reviews['ReviewBody_tokenized'] = df_reviews['ReviewBody'].apply(\n",
        "    lambda x: ' '.join(x))\n",
        "    # Fit the TF-IDF vectorizer to the text data\n",
        "    X = tfidf.fit_transform(df_reviews['ReviewBody_tokenized'])\n",
        "    # Run LDA\n",
        "    lda_model_with_tfidf = LatentDirichletAllocation(\n",
        "    n_components=6, # define number of topics\n",
        "    random_state=123)\n",
        "    lda_model_with_tfidf.fit_transform(X)\n",
        "\n",
        "    n_top_words = 10 # define numbers of words per topics\n",
        "    feature_names_tfidf = tfidf.get_feature_names_out() # extract features names from TFIDF\n",
        "\n",
        "    # Initialize a dictionary to store dominant words per LDA topic group\n",
        "    topics_dict = {}\n",
        "\n",
        "    for topic_idx, topic in enumerate(lda_model_with_tfidf.components_):\n",
        "        topic_words = [feature_names_tfidf[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
        "        topics_dict[f'Topic {(topic_idx + 1)}'] = topic_words\n",
        "\n",
        "    # Turn the dict of topics/words into df\n",
        "    lda_tfidf_topics_df = pd.DataFrame(topics_dict)\n",
        "    return feature_names_tfidf, lda_model_with_tfidf, lda_tfidf_topics_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNbfQBJLNGDm"
      },
      "outputs": [],
      "source": [
        "feature_names_tfidf, lda_model_with_tfidf, lda_tfidf_topics_df = apply_lda_with_tfidf(df_reviews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "wFpozfO-NGDm",
        "outputId": "1807f322-a33e-427f-9b29-ff430f96efcd"
      },
      "outputs": [],
      "source": [
        "visualize_topics(lda_model_with_tfidf.components_, feature_names_tfidf, n_top_words=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piIQOrNWmRA6"
      },
      "source": [
        "## Topics from LDA and TFIDF Method\n",
        "1) Lounge Experience\n",
        "\n",
        "2) Bookings\n",
        "\n",
        "3) Departures, Ticketing and Cancellations\n",
        "\n",
        "4) Luggage Handling and Delays\n",
        "\n",
        "5) Cabin and Crew Experience\n",
        "\n",
        "6) Baggage and Boarding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UqJlq_BtCMn"
      },
      "source": [
        "## **Final list of Topics based on business judgment:**\n",
        "\n",
        "1) Boarding and Crew Experience\n",
        "\n",
        "2) Entertainment and Food\n",
        "\n",
        "3) Cabin Comfort and Baggage\n",
        "\n",
        "4) Lounge Experience\n",
        "\n",
        "5) Bookings and Refunds\n",
        "\n",
        "6) Flights and Cancellations"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
