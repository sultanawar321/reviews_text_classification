{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook covers the application of topic modeling on customer reviews using LDA model. \n",
    "\n",
    "- 1) First approach using LDA from gensim with bag of words text representation technique\n",
    "\n",
    "- 2) Second approach using LDA from scikit-learn and TFIDF text representation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from google.colab import files\n",
    "\n",
    "# Natural Language Processing Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# SKlearn Libraries\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Gensim Libraries\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Visualisation libraries\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Downloading NLTK Packages\n",
    "nltk.download([\"stopwords\", \"wordnet\", \"punkt\"])\n",
    "\n",
    "# Define Stop Words\n",
    "stop_words = list(stopwords.words(\"english\"))\n",
    "\n",
    "# Add Custom Stop Words\n",
    "new_words = [\n",
    "    \"british\",\n",
    "    \"airway\",\n",
    "    \"company\",\n",
    "    \"airline\",\n",
    "    \"flight\",\n",
    "    \"heathrow\",\n",
    "    \"service\",\n",
    "    \"london\",\n",
    "    \"business\",\n",
    "    \"economy\",\n",
    "    \"customer\",\n",
    "    \"passenger\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "]\n",
    "stop_words.extend(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfkJjjoXNGDf"
   },
   "source": [
    "### 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vu7yh7ffkFcf"
   },
   "outputs": [],
   "source": [
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    The function:\n",
    "    - uploads the data from local machine\n",
    "    - read the data from csv file to df\n",
    "    - samples 20% of the data\n",
    "    \"\"\"\n",
    "    # Prompt user to upload a the reviews csv file\n",
    "    uploaded = files.upload()\n",
    "    # Read the data from csv\n",
    "    data = pd.read_csv(\"reviews.csv\")\n",
    "    data = data[[\"ReviewBody\"]]\n",
    "    # Sample 20% of the data\n",
    "    data = data.sample(frac=0.20, random_state=42).reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "7B-uvR3jjPyI",
    "outputId": "f9d0f76d-b205-451f-bc65-ea8ba4f750a3"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df_reviews = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBPQic7RNGDi"
   },
   "source": [
    "### 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oosPfw9Wj6ii"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> list:\n",
    "    \"\"\"\n",
    "    This function prepares the text data, conducting the following steps:\n",
    "    1) Removal of text in sqaure brackets\n",
    "    2) Removal of words containing numbers\n",
    "    3) Tokenization\n",
    "    4) Lemmatization\n",
    "    5) Removal of stopwords\n",
    "    6) Removal of punctuation and short words\n",
    "    \"\"\"\n",
    "    # Remove text in square brackets\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    # Remove special characters such as emojis\n",
    "    text = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", text)\n",
    "    # Remove words containing numbers\n",
    "    text = re.sub(r\"\\b\\w*\\d\\w*\\b\", \"\", text)\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Initialize WordNet lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Lemmatize tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Remove stop words and punctuation\n",
    "    processed_tokens = [\n",
    "        token\n",
    "        for token in lemmatized_tokens\n",
    "        if token.lower() not in stop_words\n",
    "        and token not in string.punctuation\n",
    "        and len(token) > 5\n",
    "    ]\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJFW5nXrk9UA"
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "df_reviews[\"ReviewBody\"] = df_reviews.ReviewBody.apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcRZ8fpbNGDj"
   },
   "source": [
    "### 3. Bag of words (BoW) and LDA (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKsYNQZ_NGDk"
   },
   "outputs": [],
   "source": [
    "def apply_lda_with_bag_of_words(df_reviews: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function applies LDA from gensim with bag of words approach.\n",
    "    \"\"\"\n",
    "    # Map IDs to words to be used as an input for the LDA model using the universal corpous ids\n",
    "    words = corpora.Dictionary(df_reviews[\"ReviewBody\"])\n",
    "\n",
    "    # Turn each review into a bag of words.\n",
    "    corpus = [words.doc2bow(doc) for doc in df_reviews[\"ReviewBody\"]]\n",
    "\n",
    "    # Apply the LDA model from gensim to establish topics\n",
    "    lda_bag_of_words_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,  # text\n",
    "        id2word=words,  # representations\n",
    "        num_topics=6,  # define number of topics\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    feature_names_bag_of_words = list(\n",
    "        words.values()\n",
    "    )  # get the features names from Bag of Words\n",
    "    n_top_words = 10\n",
    "\n",
    "    # Initialize a dictionary to store dominant words per LDA topic group\n",
    "    topics_dict = {}\n",
    "\n",
    "    for topic_idx, topic in enumerate(lda_bag_of_words_model.get_topics()):\n",
    "        topic_words = [\n",
    "            feature_names_bag_of_words[i]\n",
    "            for i in topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        ]\n",
    "        topics_dict[f\"Topic {(topic_idx + 1)}\"] = topic_words\n",
    "\n",
    "    # Turn the dict of topics/words into df\n",
    "    lda_bag_of_words_model_topics_df = pd.DataFrame(topics_dict)\n",
    "\n",
    "    return (\n",
    "        feature_names_bag_of_words,\n",
    "        lda_bag_of_words_model,\n",
    "        lda_bag_of_words_model_topics_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YYhdSUd2NGDk",
    "outputId": "19628066-ef59-4ca3-e01b-b40db1d2a203"
   },
   "outputs": [],
   "source": [
    "# Apply Gensim LDA with BoW approach\n",
    "(\n",
    "    feature_names_bag_of_words,\n",
    "    lda_bag_of_words_model,\n",
    "    lda_with_bag_of_words_topics_df,\n",
    ") = apply_lda_with_bag_of_words(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cd_rJod29AXF"
   },
   "outputs": [],
   "source": [
    "def visualize_topics(model_method: str, feature_names: list, n_top_words: int) -> None:\n",
    "    \"\"\"\n",
    "    Visualizes topics generated by a topic modeling method using word clouds.\n",
    "    \"\"\"\n",
    "    # Create subplots for each topic\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=2, ncols=3, figsize=(16, 8), sharex=True, sharey=True\n",
    "    )\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for topic_idx, topic in enumerate(model_method):\n",
    "        # Generate word cloud for each topic\n",
    "        wordcloud = WordCloud(background_color=\"white\", colormap=\"viridis\").generate(\n",
    "            \" \".join(\n",
    "                [feature_names[i] for i in topic.argsort()[: -n_top_words - 1 : -1]]\n",
    "            )\n",
    "        )\n",
    "        ax = axes[topic_idx]\n",
    "        ax.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        ax.set_title(f\"Topic {topic_idx + 1}\", fontsize=16)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "lKLXpxuzNGDl",
    "outputId": "cb615ebf-bba5-4de7-81df-26ae09073971"
   },
   "outputs": [],
   "source": [
    "# Visualise Gensim LDA with BoW topics\n",
    "visualize_topics(\n",
    "    lda_bag_of_words_model.get_topics(), feature_names_bag_of_words, n_top_words=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdJQP631h8Fc"
   },
   "source": [
    "## Topics from LDA (Gensim) and BoW Method\n",
    "- Lounge\n",
    "\n",
    "- Crew experience\n",
    "\n",
    "- Flight returns and refunds\n",
    "\n",
    "- Luggage\n",
    "\n",
    "- Boarding\n",
    "\n",
    "- Departures and flight times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HTlkERdNGDm"
   },
   "source": [
    "### 4. TFIDF and LDA (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DW4hpB1RNGDm"
   },
   "outputs": [],
   "source": [
    "def apply_lda_with_tfidf(df_reviews):\n",
    "    \"\"\"\n",
    "    This function applies LDA from scikit-learn with tfidf approach.\n",
    "    \"\"\"\n",
    "    # Initialize TFIDF\n",
    "    tfidf = TfidfVectorizer(max_df=0.8, min_df=20, max_features=10000)\n",
    "    # Convert the text column to a list of strings\n",
    "    df_reviews[\"ReviewBody_tokenized\"] = df_reviews[\"ReviewBody\"].apply(\n",
    "        lambda x: \" \".join(x)\n",
    "    )\n",
    "    # Fit the TF-IDF vectorizer to the text data\n",
    "    X = tfidf.fit_transform(df_reviews[\"ReviewBody_tokenized\"])\n",
    "    # Run LDA\n",
    "    lda_model_with_tfidf = LatentDirichletAllocation(\n",
    "        n_components=6, random_state=123  # define number of topics\n",
    "    )\n",
    "    lda_model_with_tfidf.fit_transform(X)\n",
    "\n",
    "    n_top_words = 10  # define numbers of words per topics\n",
    "    feature_names_tfidf = (\n",
    "        tfidf.get_feature_names_out()\n",
    "    )  # extract features names from TFIDF\n",
    "\n",
    "    # Initialize a dictionary to store dominant words per LDA topic group\n",
    "    topics_dict = {}\n",
    "\n",
    "    for topic_idx, topic in enumerate(lda_model_with_tfidf.components_):\n",
    "        topic_words = [\n",
    "            feature_names_tfidf[i] for i in topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        ]\n",
    "        topics_dict[f\"Topic {(topic_idx + 1)}\"] = topic_words\n",
    "\n",
    "    # Turn the dict of topics/words into df\n",
    "    lda_tfidf_topics_df = pd.DataFrame(topics_dict)\n",
    "    return feature_names_tfidf, lda_model_with_tfidf, lda_tfidf_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNbfQBJLNGDm"
   },
   "outputs": [],
   "source": [
    "# Apply Sklearn LDA with TFIDF approach\n",
    "feature_names_tfidf, lda_model_with_tfidf, lda_tfidf_topics_df = apply_lda_with_tfidf(\n",
    "    df_reviews\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "wFpozfO-NGDm",
    "outputId": "edce203d-c202-43ec-a8f2-6c56eee29ae6"
   },
   "outputs": [],
   "source": [
    "# Visualise Sklearn LDA with TFIDF topics\n",
    "visualize_topics(lda_model_with_tfidf.components_, feature_names_tfidf, n_top_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piIQOrNWmRA6"
   },
   "source": [
    "## Topics from LDA and TFIDF Method\n",
    "\n",
    "- Boarding\n",
    "\n",
    "- Cabin and crew experience\n",
    "\n",
    "- Entertaiment\n",
    "\n",
    "- Flight times\n",
    "\n",
    "- Bookings, refunds, and cacellations\n",
    "\n",
    "- Luggage handling and delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UqJlq_BtCMn"
   },
   "source": [
    "## **Final list of Topics based on business judgment:**\n",
    "\n",
    "1) Boarding and Crew Experience\n",
    "\n",
    "2) Entertainment and Food\n",
    "\n",
    "3) Cabin Comfort and Baggage\n",
    "\n",
    "4) Lounge Experience\n",
    "\n",
    "5) Bookings and Refunds\n",
    "\n",
    "6) Flights and Cancellations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
