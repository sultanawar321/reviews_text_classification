"""A Python module which applies data preprocessing and topic modelling using LDA algorthim"""
# Standard Libraries
import os
import re
import string
import pandas as pd

# Natural Language Processing Libraries
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# Machine Learning Libraries
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import TfidfVectorizer

# Visualisation libraries
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Downloading NLTK Packages
# nltk.download(['stopwords', 'wordnet', 'punkt'])

# Define Stop Words
stop_words = list(stopwords.words("english"))

# Add Custom Stop Words
new_words = [
    "british",
    "airway",
    "company",
    "airline",
    "flight",
    "heathrow",
    "service",
    "london",
    "business",
    "economy",
    "customer",
    "passenger",
    "hour",
    "minute",
]
stop_words.extend(new_words)


def load_data() -> pd.DataFrame:
    """
    This function loads the data from a CSV file and samples 20% of the data.
    """
    data = pd.read_csv(os.path.join("..", "data", "reviews.csv"))
    data = data[["ReviewBody"]]
    data = data.sample(frac=0.20, random_state=42).reset_index(drop=True)
    return data


def preprocess_text(text: str, stop_words: list) -> list:
    """
    This function prepares the text data by conducting several preprocessing steps.
    """

    # Remove text in square brackets
    text = re.sub(r"\[.*?\]", "", text)
    # Remove special characters
    text = re.sub(r"[^a-zA-Z0-9 ]", "", text)
    # Remove words containing numbers
    text = re.sub(r"\b\w*\d\w*\b", "", text)
    # Remove emojis
    text = text.encode("ascii", "ignore").decode("ascii")
    # Remove extra spaces and newline characters
    text = re.sub(r"\s+", " ", text).strip()
    # Tokenize the text
    tokens = word_tokenize(text.lower())
    # Initialize WordNet lemmatizer
    lemmatizer = WordNetLemmatizer()
    # Lemmatize tokens
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    # Remove stop words, punctuation, and short words
    processed_tokens = [
        token
        for token in lemmatized_tokens
        if token.lower() not in stop_words
        and token not in string.punctuation
        and len(token) > 5
    ]
    return processed_tokens


def apply_lda_with_tfidf(df_reviews):
    """
    This function applies LDA from scikit-learn with the TF-IDF approach.
    """
    # Initialize TF-IDF
    tfidf = TfidfVectorizer(max_df=0.8, min_df=20, max_features=10000)
    # Convert the ReviewBody column to a list of strings
    df_reviews["ReviewBody_tokenized"] = df_reviews["ReviewBody"].apply(
        lambda x: " ".join(x)
    )
    # Fit the TF-IDF vectorizer to the data
    X = tfidf.fit_transform(df_reviews["ReviewBody_tokenized"])
    # Run LDA
    lda_model_with_tfidf = LatentDirichletAllocation(
        n_components=6, random_state=123, learning_method="batch"
    )
    lda_model_with_tfidf.fit_transform(X)

    n_top_words = 10

    feature_names_tfidf = tfidf.get_feature_names()

    # Initialize a dictionary to store topic words
    topics_dict = {}

    for topic_idx, topic in enumerate(lda_model_with_tfidf.components_):
        topic_words = [
            feature_names_tfidf[i] for i in topic.argsort()[: -n_top_words - 1 : -1]
        ]
        topics_dict[f"Topic {(topic_idx + 1)}"] = topic_words

    # Turn the dictionary into a DataFrame
    lda_tfidf_topics_df = pd.DataFrame(topics_dict)
    return feature_names_tfidf, lda_model_with_tfidf, lda_tfidf_topics_df


def visualize_topics(
    model_method: np.array, feature_names: list, n_top_words: int
) -> None:
    """
    Visualizes topics generated by a topic modeling method using word clouds.
    """
    # Create subplots for each topic
    fig, axes = plt.subplots(
        nrows=2, ncols=3, figsize=(20, 8), sharex=True, sharey=True
    )
    axes = axes.flatten()

    for topic_idx, topic in enumerate(model_method):
        # Generate a word cloud for each topic
        wordcloud = WordCloud(background_color="white", colormap="viridis").generate(
            " ".join(
                [feature_names[i] for i in topic.argsort()[: -n_top_words - 1 : -1]]
            )
        )
        ax = axes[topic_idx]
        ax.imshow(wordcloud, interpolation="bilinear")
        ax.set_title(f"Topic {topic_idx + 1}", fontsize=16)
        ax.axis("off")

    plt.tight_layout()
    plt.show()


def main():
    df_reviews = load_data()
    df_reviews["ReviewBody"] = df_reviews["ReviewBody"].apply(
        lambda x: preprocess_text(x, stop_words)
    )
    (
        feature_names_tfidf,
        lda_model_with_tfidf,
        lda_tfidf_topics_df,
    ) = apply_lda_with_tfidf(df_reviews)
    print(lda_tfidf_topics_df)
    print(len(feature_names_tfidf))
    print(len(lda_model_with_tfidf))


if __name__ == "__main__":
    main()
